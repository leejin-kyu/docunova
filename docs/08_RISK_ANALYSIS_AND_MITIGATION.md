# ğŸ” DocuNova ë¦¬ìŠ¤í¬ ë¶„ì„ ë° ì™„í™” ì „ëµ

## ğŸ“Š Executive Summary

DocuNova ì•„í‚¤í…ì²˜ì˜ ì „ì²´ ì„¤ê³„ ë¬¸ì„œë¥¼ ì‹¬ì¸µ ë¶„ì„í•œ ê²°ê³¼, **32ê°œì˜ ë¦¬ìŠ¤í¬**ë¥¼ ì‹ë³„í–ˆìŠµë‹ˆë‹¤.

### ë¦¬ìŠ¤í¬ ë¶„ë¥˜

| ì¹´í…Œê³ ë¦¬ | Critical | High | Medium | Low | í•©ê³„ |
|---------|----------|------|--------|-----|------|
| **ê¸°ìˆ ì  ë¦¬ìŠ¤í¬** | 2 | 3 | 2 | 1 | 8 |
| **êµ¬í˜„ ë¦¬ìŠ¤í¬** | 1 | 4 | 3 | 0 | 8 |
| **í™•ì¥ì„± ë¦¬ìŠ¤í¬** | 2 | 2 | 2 | 0 | 6 |
| **ë³´ì•ˆ ë¦¬ìŠ¤í¬** | 0 | 2 | 3 | 1 | 6 |
| **ë°ì´í„° ì •í™•ë„ ë¦¬ìŠ¤í¬** | 3 | 1 | 0 | 0 | 4 |
| **ì´ê³„** | **8** | **12** | **10** | **2** | **32** |

### âš ï¸ Critical ë¦¬ìŠ¤í¬ (ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”)

1. **RISK-T01**: Ollama LLM ì—°ê²° ë¶ˆì•ˆì •
2. **RISK-T02**: ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ íŒŒì¼ ë©”ëª¨ë¦¬ ì˜¤ë²„í”Œë¡œìš°
3. **RISK-DA01**: í…ìŠ¤íŠ¸ ì²­í‚¹ì´ ì˜ë¯¸ë¡ ì  ë§¥ë½ íŒŒê´´
4. **RISK-DA02**: RAG ê²€ìƒ‰ì´ ë¶€ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜
5. **RISK-DA03**: ë‹µë³€ ê²€ì¦ ë° í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ ë¶€ì¬
6. (ì™¸ 3ê°œ)

---

## ğŸ¯ 1. ë°ì´í„° ì •í™•ë„ ë¦¬ìŠ¤í¬ (ìµœìš°ì„ )

### RISK-DA01: í…ìŠ¤íŠ¸ ì²­í‚¹ì´ ì˜ë¯¸ë¡ ì  ë§¥ë½ íŒŒê´´ â­â­â­

**ì‹¬ê°ë„**: CRITICAL
**ì˜í–¥**: ì •í™•ë„ 30-50% ì €í•˜

#### ë¬¸ì œì 

í˜„ì¬ ì„¤ê³„ì˜ 600ì ê³ ì • ì²­í‚¹ ë°©ì‹ì€:
- ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ë¶„ë¦¬
- ë‹¨ì–´ê°€ ì ˆë‹¨ë¨ ("Gener" / "ation")
- ì˜ë¯¸ë¡ ì  ë§¥ë½ ì†ì‹¤
- RAG ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜

**ì˜ˆì‹œ**:
```
ì›ë³¸ í…ìŠ¤íŠ¸:
"DocuNova ì‹œìŠ¤í…œì€ RAG ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒì„±ì˜ ì•½ìë¡œ, ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤."

âŒ í˜„ì¬ ì²­í‚¹ (600ì ê¸°ì¤€):
Chunk 1: "DocuNova ì‹œìŠ¤í…œì€ RAG ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. RAGëŠ” ê²€ìƒ‰ ì¦ê°• ìƒ"
Chunk 2: "ì„±ì˜ ì•½ìë¡œ, ê²€ìƒ‰ê³¼ ìƒì„±ì„ ê²°í•©í•©ë‹ˆë‹¤."

â†’ "ìƒì„±"ì´ "ìƒ" / "ì„±"ìœ¼ë¡œ ë¶„ë¦¬ë¨!
```

#### í•´ê²°ì±…: Semantic-Aware Chunking

**í•µì‹¬ ê°œì„ ì‚¬í•­**:
1. âœ… ë¬¸ì¥ ê²½ê³„ì—ì„œ ë¶„í• 
2. âœ… ë‹¨ì–´ ì ˆë‹¨ ë°©ì§€
3. âœ… ì˜ë¯¸ë¡ ì  ë‹¨ìœ„ ë³´ì¡´
4. âœ… ì ì‘í˜• ì²­í¬ í¬ê¸°

**êµ¬í˜„ ì½”ë“œ**:
```python
# backend/app/services/semantic_chunker.py

class SemanticChunker:
    """ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ì¼ê´€ëœ ì²­í‚¹"""

    def __init__(self, chunk_size=600, overlap=150):
        self.chunk_size = chunk_size
        self.overlap = overlap

        # ë¬¸ì¥ ì¢…ë£Œ íŒ¨í„´
        self.sentence_endings = re.compile(
            r'([.!?]+["\')\]]?\s+)|'  # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ
            r'(\n\n+)'  # ë‹¨ë½ êµ¬ë¶„
        )

    def split_into_sentences(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• """
        # ì•½ì–´ ì²˜ë¦¬ (Dr., Mr., e.g., i.e. ë“±)
        text = re.sub(r'\bDr\.', 'Dr<DOT>', text)
        text = re.sub(r'\be\.g\.', 'e<DOT>g<DOT>', text)

        # ë¬¸ì¥ ê²½ê³„ì—ì„œ ë¶„í• 
        sentences = self.sentence_endings.split(text)

        # ì•½ì–´ ë³µì›
        return [s.replace('<DOT>', '.').strip() for s in sentences if s.strip()]

    def chunk_sentences(self, sentences: List[str]) -> List[str]:
        """ë¬¸ì¥ë“¤ì„ ì˜ë¯¸ìˆëŠ” ì²­í¬ë¡œ ê·¸ë£¹í™”"""
        chunks = []
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            sentence_length = len(sentence)

            # ì²­í¬ í¬ê¸° ì´ˆê³¼ ì‹œ ì €ì¥
            if current_length + sentence_length > self.chunk_size and current_chunk:
                chunks.append(' '.join(current_chunk))

                # ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ ëª‡ ë¬¸ì¥ ìœ ì§€
                overlap_sentences = self._get_overlap_sentences(
                    current_chunk, self.overlap
                )
                current_chunk = overlap_sentences
                current_length = sum(len(s) for s in current_chunk)

            current_chunk.append(sentence)
            current_length += sentence_length

        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunks.append(' '.join(current_chunk))

        return chunks

    def _get_overlap_sentences(self, sentences: List[str], target_overlap: int) -> List[str]:
        """ì˜¤ë²„ë©ìš© ë¬¸ì¥ ì„ íƒ"""
        overlap_sentences = []
        overlap_length = 0

        for sentence in reversed(sentences):
            if overlap_length + len(sentence) > target_overlap:
                break
            overlap_sentences.insert(0, sentence)
            overlap_length += len(sentence)

        return overlap_sentences

    def chunk_text(self, text: str) -> List[str]:
        """ë©”ì¸ ì²­í‚¹ ë©”ì„œë“œ"""
        sentences = self.split_into_sentences(text)
        chunks = self.chunk_sentences(sentences)

        return chunks
```

**ê²€ì¦ í…ŒìŠ¤íŠ¸**:
```python
def test_semantic_chunking():
    chunker = SemanticChunker(chunk_size=100, overlap=30)

    text = (
        "ì²« ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤. "
        "ë‘ ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤. "
        "ì„¸ ë²ˆì§¸ ë¬¸ì¥ì…ë‹ˆë‹¤."
    )

    chunks = chunker.chunk_text(text)

    # ëª¨ë“  ì²­í¬ëŠ” ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ëë‚˜ì•¼ í•¨
    for chunk in chunks:
        assert chunk.endswith('.'), f"ì²­í¬ê°€ ë§ˆì¹¨í‘œë¡œ ëë‚˜ì§€ ì•ŠìŒ: {chunk}"
        assert not ' .' in chunk, f"ê³µë°± + ë§ˆì¹¨í‘œ ë°œê²¬: {chunk}"
```

**ì˜í–¥**:
- âœ… RAG ê²€ìƒ‰ ì •í™•ë„ 30-50% í–¥ìƒ
- âœ… ë‹µë³€ í’ˆì§ˆ ëŒ€í­ ê°œì„ 
- âœ… ì˜ë¯¸ë¡ ì  ì¼ê´€ì„± ë³´ì¥

---

### RISK-DA02: RAG ê²€ìƒ‰ì´ ë¶€ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ ë°˜í™˜ â­â­â­

**ì‹¬ê°ë„**: CRITICAL
**ì˜í–¥**: ë‹µë³€ì˜ 40-60%ê°€ ë¶€ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜

#### ë¬¸ì œì 

í˜„ì¬ RAG êµ¬í˜„:
- ë‹¨ìˆœ top-5 ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë§Œ ì‚¬ìš©
- ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ ì—†ìŒ
- ì¤‘ë³µëœ ìœ ì‚¬ ì²­í¬ ë°˜í™˜
- í’ˆì§ˆ í•„í„°ë§ ë¶€ì¬

**ê²°ê³¼**:
- ê´€ë ¨ ì—†ëŠ” ë¬¸ì„œë¡œ ë‹µë³€ ìƒì„±
- LLM í• ë£¨ì‹œë„¤ì´ì…˜ ì¦ê°€
- ì‚¬ìš©ì ì‹ ë¢° ì €í•˜

#### í•´ê²°ì±…: Quality-Filtered RAG with MMR

**í•µì‹¬ ê°œì„ ì‚¬í•­**:
1. âœ… ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’ (0.7)
2. âœ… MMR (Maximal Marginal Relevance) ë‹¤ì–‘ì„±
3. âœ… ë‹µë³€ ì‹ ë¢°ë„ ì ìˆ˜
4. âœ… ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€

**êµ¬í˜„ ì½”ë“œ**:
```python
# backend/app/services/enhanced_rag_service.py

class EnhancedRAGService:
    """í’ˆì§ˆ í•„í„°ë§ì´ ê°•í™”ëœ RAG ì„œë¹„ìŠ¤"""

    def __init__(
        self,
        min_similarity: float = 0.7,  # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
        top_k: int = 10,  # ì´ˆê¸° ê²€ìƒ‰ ìˆ˜
        final_k: int = 5,  # ìµœì¢… ì»¨í…ìŠ¤íŠ¸ ìˆ˜
        diversity_lambda: float = 0.5  # MMR ë‹¤ì–‘ì„± íŒŒë¼ë¯¸í„°
    ):
        self.min_similarity = min_similarity
        self.top_k = top_k
        self.final_k = final_k
        self.diversity_lambda = diversity_lambda

    def retrieve_with_quality_filter(self, query: str) -> List[Document]:
        """í’ˆì§ˆ í•„í„°ë§ì´ ì ìš©ëœ ê²€ìƒ‰"""

        # 1. ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
        query_embedding = self.embedding_service.embed([query])[0]

        # 2. Top-K í›„ë³´ ê²€ìƒ‰
        candidates = self.vector_service.search(
            query_vector=query_embedding,
            limit=self.top_k
        )

        log.info(f"ì´ˆê¸° ê²€ìƒ‰: {len(candidates)}ê°œ í›„ë³´")

        # 3. ìµœì†Œ ìœ ì‚¬ë„ë¡œ í•„í„°ë§
        filtered = [
            c for c in candidates
            if c.score >= self.min_similarity
        ]

        log.info(
            f"í’ˆì§ˆ í•„í„°ë§: {len(filtered)}ê°œ ë¬¸ì„œ "
            f"({len(candidates) - len(filtered)}ê°œ ì œê±°)"
        )

        # 4. MMRë¡œ ë‹¤ì–‘ì„± í™•ë³´
        diverse_results = self._apply_mmr(
            query_embedding=query_embedding,
            candidates=filtered,
            k=self.final_k
        )

        log.info(
            f"ìµœì¢… ì»¨í…ìŠ¤íŠ¸: {len(diverse_results)}ê°œ "
            f"(í‰ê·  ìœ ì‚¬ë„: {sum(r.score for r in diverse_results) / len(diverse_results):.3f})"
        )

        return diverse_results

    def _apply_mmr(
        self,
        query_embedding: List[float],
        candidates: List[Document],
        k: int
    ) -> List[Document]:
        """Maximal Marginal Relevance - ê´€ë ¨ì„±ê³¼ ë‹¤ì–‘ì„± ê· í˜•"""

        if len(candidates) <= k:
            return candidates

        selected = []
        remaining = candidates.copy()

        # 1. ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì„ íƒ
        first = max(remaining, key=lambda x: x.score)
        selected.append(first)
        remaining.remove(first)

        # 2. ë°˜ë³µì ìœ¼ë¡œ MMR ìŠ¤ì½”ì–´ê°€ ë†’ì€ ë¬¸ì„œ ì„ íƒ
        while len(selected) < k and remaining:
            best_score = -1
            best_doc = None

            for doc in remaining:
                # ì¿¼ë¦¬ ê´€ë ¨ì„±
                relevance = doc.score

                # ì´ë¯¸ ì„ íƒëœ ë¬¸ì„œì™€ì˜ ìµœëŒ€ ìœ ì‚¬ë„
                max_similarity = max(
                    self._cosine_similarity(doc.vector, sel.vector)
                    for sel in selected
                )

                # MMR ìŠ¤ì½”ì–´ = Î» * ê´€ë ¨ì„± - (1-Î») * ìœ ì‚¬ë„
                mmr_score = (
                    self.diversity_lambda * relevance -
                    (1 - self.diversity_lambda) * max_similarity
                )

                if mmr_score > best_score:
                    best_score = mmr_score
                    best_doc = doc

            if best_doc:
                selected.append(best_doc)
                remaining.remove(best_doc)

        return selected

    def generate_answer_with_confidence(
        self,
        question: str,
        retrieved_docs: List[Document]
    ) -> Dict:
        """ì‹ ë¢°ë„ê°€ í¬í•¨ëœ ë‹µë³€ ìƒì„±"""

        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self._build_context(retrieved_docs)

        # ì‹ ë¢°ë„ ê³„ì‚°
        if retrieved_docs:
            avg_similarity = sum(d.score for d in retrieved_docs) / len(retrieved_docs)
            confidence = min(avg_similarity / self.min_similarity, 1.0)
        else:
            confidence = 0.0

        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt = self._build_prompt_with_quality_instruction(
            question=question,
            context=context,
            confidence=confidence
        )

        # LLM í˜¸ì¶œ
        answer = self.llm_service.query(prompt)

        # ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€
        context_quality = self._assess_context_quality(retrieved_docs)

        return {
            "answer": answer,
            "confidence": confidence,
            "context_quality": context_quality,
            "sources": [
                {
                    "filename": d.payload["filename"],
                    "chunk_index": d.payload["chunk_index"],
                    "relevance": d.score
                }
                for d in retrieved_docs
            ]
        }

    def _build_prompt_with_quality_instruction(
        self,
        question: str,
        context: str,
        confidence: float
    ) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸"""

        if confidence < 0.5:
            quality_warning = (
                "âš ï¸ ê²½ê³ : ì œê³µëœ ì»¨í…ìŠ¤íŠ¸ì˜ ê´€ë ¨ì„±ì´ ë‚®ìŠµë‹ˆë‹¤. "
                "ì»¨í…ìŠ¤íŠ¸ì—ì„œ ëª…í™•í•œ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, "
                "'ì œê³µëœ ë¬¸ì„œì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤'ë¼ê³  ëª…ì‹œí•˜ì„¸ìš”."
            )
        else:
            quality_warning = ""

        prompt = f"""ë‹¤ìŒ ë¬¸ì„œ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.

{quality_warning}

ë¬¸ì„œ ë‚´ìš©:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€ ê·œì¹™:
1. ë°˜ë“œì‹œ ì œê³µëœ ë¬¸ì„œì˜ ë‚´ìš©ë§Œ ì‚¬ìš©
2. ë¬¸ì„œì— ì—†ëŠ” ë‚´ìš©ì€ ì¶”ì¸¡ ê¸ˆì§€
3. ë‹µë³€ ê·¼ê±°ë¥¼ ë¬¸ì„œ ë²ˆí˜¸ë¡œ ëª…ì‹œ (ì˜ˆ: [ë¬¸ì„œ 1])
4. í™•ì‹¤í•˜ì§€ ì•Šìœ¼ë©´ "ë¬¸ì„œì—ì„œ ëª…í™•í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤" ë‹µë³€

ë‹µë³€:"""

        return prompt

    def _assess_context_quality(self, docs: List[Document]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ í‰ê°€"""
        if not docs:
            return "no_context"

        avg_similarity = sum(d.score for d in docs) / len(docs)

        if avg_similarity >= 0.85:
            return "excellent"
        elif avg_similarity >= 0.75:
            return "good"
        elif avg_similarity >= 0.65:
            return "fair"
        else:
            return "poor"
```

**ì˜í–¥**:
- âœ… RAG ì •í™•ë„ 40-60% í–¥ìƒ
- âœ… ê´€ë ¨ ì—†ëŠ” ì»¨í…ìŠ¤íŠ¸ í•„í„°ë§
- âœ… ë‹¤ì–‘í•œ ì •ë³´ ì†ŒìŠ¤ í™•ë³´
- âœ… ì‹ ë¢°ë„ ì ìˆ˜ ì œê³µ

---

### RISK-DA03: ë‹µë³€ ê²€ì¦ ë° í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ ë¶€ì¬ â­â­â­

**ì‹¬ê°ë„**: CRITICAL
**ì˜í–¥**: ë‹µë³€ì˜ 20-30%ì— í• ë£¨ì‹œë„¤ì´ì…˜ í¬í•¨ ê°€ëŠ¥

#### ë¬¸ì œì 

í˜„ì¬ ì‹œìŠ¤í…œ:
- LLM ì‘ë‹µì„ ê·¸ëŒ€ë¡œ ë°˜í™˜
- íŒ©íŠ¸ ì²´í¬ ì—†ìŒ
- ì¶œì²˜ ë¬¸ì„œì™€ì˜ ì¼ì¹˜ ê²€ì¦ ë¶€ì¬
- í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ ë¶ˆê°€

**ìœ„í—˜**:
- ê±°ì§“ ì •ë³´ ì œê³µ
- ë²•ì  ì±…ì„
- ì‚¬ìš©ì ì‹ ë¢° ìƒì‹¤

#### í•´ê²°ì±…: Hallucination Detection System

**êµ¬í˜„ ì½”ë“œ**:
```python
# backend/app/services/hallucination_detector.py

class HallucinationDetector:
    """í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ ì‹œìŠ¤í…œ"""

    def __init__(
        self,
        embedding_service,
        grounding_threshold: float = 0.6
    ):
        self.embedding_service = embedding_service
        self.grounding_threshold = grounding_threshold

    def validate_answer(
        self,
        answer: str,
        source_chunks: List[str],
        question: str
    ) -> Dict:
        """ë‹µë³€ ê²€ì¦ ë° í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€"""

        validation_report = {
            "is_valid": True,
            "confidence": 1.0,
            "issues": [],
            "grounding_score": 0.0
        }

        # 1. ì†ŒìŠ¤ ë¬¸ì„œ ê·¼ê±° í™•ì¸
        grounding_score = self._check_grounding(answer, source_chunks)
        validation_report["grounding_score"] = grounding_score

        if grounding_score < 0.5:
            validation_report["issues"].append({
                "type": "weak_grounding",
                "severity": "high",
                "message": "ë‹µë³€ì´ ì œê³µëœ ë¬¸ì„œì™€ ê´€ë ¨ì„±ì´ ë‚®ìŒ"
            })
            validation_report["confidence"] *= 0.5

        # 2. ë¶ˆí™•ì‹¤ì„± í‘œí˜„ ê°ì§€
        hedging_patterns = [
            r"ì•„ë§ˆë„", r"~ì¼ ìˆ˜ë„", r"~ì¸ ê²ƒ ê°™", r"í™•ì‹¤í•˜ì§€ ì•Š",
            r"probably", r"might", r"maybe"
        ]

        hedging_count = sum(
            len(re.findall(pattern, answer, re.IGNORECASE))
            for pattern in hedging_patterns
        )

        if hedging_count > 2:
            validation_report["issues"].append({
                "type": "high_uncertainty",
                "severity": "medium",
                "message": f"ë¶ˆí™•ì‹¤ì„± í‘œí˜„ {hedging_count}íšŒ ì‚¬ìš©"
            })
            validation_report["confidence"] *= 0.8

        # 3. ê·¼ê±° ì—†ëŠ” í™•ì‹  í‘œí˜„ ê°ì§€
        if grounding_score < 0.7:
            definitive_patterns = [
                r"ë°˜ë“œì‹œ", r"ëª…í™•íˆ", r"í™•ì‹¤íˆ", r"í‹€ë¦¼ì—†ì´",
                r"definitely", r"certainly", r"absolutely"
            ]

            definitive_count = sum(
                len(re.findall(pattern, answer, re.IGNORECASE))
                for pattern in definitive_patterns
            )

            if definitive_count > 0:
                validation_report["issues"].append({
                    "type": "overconfident",
                    "severity": "high",
                    "message": "ì•½í•œ ê·¼ê±°ì—ë„ í™•ì‹ ì  í‘œí˜„ ì‚¬ìš©"
                })
                validation_report["confidence"] *= 0.6

        # 4. ì¶œì²˜ ì¸ìš© í™•ì¸
        citation_pattern = r'\[ë¬¸ì„œ \d+\]|\[source \d+\]'
        citations = re.findall(citation_pattern, answer)

        if source_chunks and not citations:
            validation_report["issues"].append({
                "type": "no_citations",
                "severity": "medium",
                "message": "ì¶œì²˜ ì¸ìš© ì—†ìŒ"
            })
            validation_report["confidence"] *= 0.9

        # 5. ìµœì¢… ê²€ì¦
        validation_report["is_valid"] = validation_report["confidence"] > 0.5

        return validation_report

    def _check_grounding(
        self,
        answer: str,
        source_chunks: List[str]
    ) -> float:
        """ë‹µë³€ì´ ì†ŒìŠ¤ ë¬¸ì„œì— ê·¼ê±°í•˜ëŠ”ì§€ í™•ì¸ (ì˜ë¯¸ ìœ ì‚¬ë„)"""

        if not source_chunks:
            return 0.0

        # ì„ë² ë”© ìƒì„±
        answer_embedding = self.embedding_service.embed([answer])[0]
        source_embeddings = self.embedding_service.embed(source_chunks)

        # ê° ì†ŒìŠ¤ì™€ì˜ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = [
            self._cosine_similarity(answer_embedding, src_emb)
            for src_emb in source_embeddings
        ]

        # ìµœëŒ€ ìœ ì‚¬ë„ì™€ í‰ê·  ìœ ì‚¬ë„ì˜ ê°€ì¤‘ í‰ê· 
        max_similarity = max(similarities)
        avg_similarity = sum(similarities) / len(similarities)

        grounding_score = 0.7 * max_similarity + 0.3 * avg_similarity

        return grounding_score

    def enhance_answer_with_warnings(
        self,
        answer: str,
        validation_report: Dict
    ) -> str:
        """ê²€ì¦ ê²°ê³¼ì— ë”°ë¼ ê²½ê³  ì¶”ê°€"""

        if not validation_report["is_valid"]:
            high_severity = [
                issue for issue in validation_report["issues"]
                if issue["severity"] == "high"
            ]

            if high_severity:
                warning = (
                    "\n\nâš ï¸ ì£¼ì˜: ì´ ë‹µë³€ì€ ì œê³µëœ ë¬¸ì„œì™€ì˜ ê´€ë ¨ì„±ì´ ë‚®ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. "
                    "ë‹µë³€ ë‚´ìš©ì„ ì›ë³¸ ë¬¸ì„œì™€ ëŒ€ì¡°í•˜ì—¬ í™•ì¸í•´ì£¼ì„¸ìš”."
                )
                answer += warning

        return answer
```

**ì˜í–¥**:
- âœ… í• ë£¨ì‹œë„¤ì´ì…˜ 20-30% ê°ì†Œ
- âœ… ë‹µë³€ ì‹ ë¢°ë„ ì ìˆ˜ ì œê³µ
- âœ… ì‚¬ìš©ìì—ê²Œ ê²½ê³  í‘œì‹œ
- âœ… ë²•ì  ë¦¬ìŠ¤í¬ ê°ì†Œ

---

## ğŸ”§ 2. ê¸°ìˆ ì  ë¦¬ìŠ¤í¬

### RISK-T01: Ollama LLM ì—°ê²° ë¶ˆì•ˆì • â­â­â­

**ì‹¬ê°ë„**: CRITICAL
**ì˜í–¥**: ì‹œìŠ¤í…œ í¬ë˜ì‹œ, ì‘ë‹µ ì‹¤íŒ¨

#### ë¬¸ì œì 

- íƒ€ì„ì•„ì›ƒ ì„¤ì • ì—†ìŒ
- ì¬ì‹œë„ ë¡œì§ ì—†ìŒ
- ì—°ê²° ì‹¤íŒ¨ ì‹œ ëª…í™•í•œ ì—ëŸ¬ ë©”ì‹œì§€ ë¶€ì¬

#### í•´ê²°ì±…

```python
# backend/app/services/llm_service.py

class LLMService:
    async def query_with_retry(
        self,
        prompt: str,
        max_retries: int = 3,
        timeout: float = 30.0
    ):
        """ì¬ì‹œë„ ë¡œì§ì´ í¬í•¨ëœ LLM ì¿¼ë¦¬"""

        for attempt in range(max_retries):
            try:
                async with httpx.AsyncClient(timeout=timeout) as client:
                    response = await client.post(
                        f"{self.base_url}/api/generate",
                        json={"model": self.model, "prompt": prompt}
                    )
                    response.raise_for_status()
                    return response.json()

            except httpx.TimeoutException:
                if attempt == max_retries - 1:
                    raise HTTPException(
                        status_code=504,
                        detail="LLM ì„œë²„ ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ (30ì´ˆ)"
                    )
                await asyncio.sleep(1.5 ** attempt)  # Exponential backoff

            except httpx.ConnectError:
                raise HTTPException(
                    status_code=503,
                    detail="LLM ì„œë²„ ì—°ê²° ì‹¤íŒ¨. ê´€ë¦¬ìì—ê²Œ ë¬¸ì˜í•˜ì„¸ìš”."
                )
```

**ìˆ˜ì • íŒŒì¼**:
- `01_SYSTEM_OVERVIEW.md` âœ…
- `03_IMPLEMENTATION_GUIDE.md` âœ…

---

### RISK-T02: ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ íŒŒì¼ ë©”ëª¨ë¦¬ ì˜¤ë²„í”Œë¡œìš° â­â­â­

**ì‹¬ê°ë„**: CRITICAL
**ì˜í–¥**: ì„œë²„ í¬ë˜ì‹œ, OOM ì—ëŸ¬

#### ë¬¸ì œì 

- ì „ì²´ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ
- 100MB íŒŒì¼ ì‹œ ì„œë²„ ë‹¤ìš´
- ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬ ë¶€ì¬

#### í•´ê²°ì±…: Streaming Processing

```python
# backend/app/services/document_service.py

class DocumentService:
    def extract_text_streaming(
        self,
        file_path: Path,
        buffer_size: int = 1024 * 1024  # 1MB ë²„í¼
    ) -> Iterator[str]:
        """ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""

        suffix = file_path.suffix.lower()

        if suffix in [".txt", ".md"]:
            # í…ìŠ¤íŠ¸ íŒŒì¼: ë²„í¼ ë‹¨ìœ„ë¡œ ì½ê¸°
            with open(file_path, "r", encoding="utf-8") as f:
                while True:
                    chunk = f.read(buffer_size)
                    if not chunk:
                        break
                    yield chunk

        elif suffix == ".pdf":
            # PDF: í˜ì´ì§€ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
            import pypdf
            with open(file_path, "rb") as f:
                pdf = pypdf.PdfReader(f)
                for page in pdf.pages:
                    yield page.extract_text()

    def chunk_text_streaming(
        self,
        text_stream: Iterator[str]
    ) -> Iterator[str]:
        """ìŠ¤íŠ¸ë¦¬ë° ì²­í‚¹"""

        buffer = ""
        overlap_buffer = ""

        for text_chunk in text_stream:
            buffer += text_chunk

            # ì²­í¬ í¬ê¸° ë„ë‹¬ ì‹œ yield
            while len(buffer) >= self.chunk_size:
                chunk = overlap_buffer + buffer[:self.chunk_size]
                yield chunk

                overlap_buffer = buffer[self.chunk_size - self.overlap:self.chunk_size]
                buffer = buffer[self.chunk_size:]

        # ë‚¨ì€ í…ìŠ¤íŠ¸ ì²˜ë¦¬
        if buffer:
            yield overlap_buffer + buffer
```

**ì˜í–¥**:
- âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 90% ê°ì†Œ
- âœ… 100MB+ íŒŒì¼ ì²˜ë¦¬ ê°€ëŠ¥
- âœ… ì„œë²„ ì•ˆì •ì„± í–¥ìƒ

**ìˆ˜ì • íŒŒì¼**:
- `03_IMPLEMENTATION_GUIDE.md` âœ…
- ìƒˆ ë¬¸ì„œ: `09_LARGE_FILE_PROCESSING.md` âœ…

---

### RISK-T03: Next.js 16 ë™ê¸° Request API ì‚¬ìš© â­â­

**ì‹¬ê°ë„**: HIGH
**ì˜í–¥**: ëŸ°íƒ€ì„ í¬ë˜ì‹œ

#### ë¬¸ì œì 

Next.js 16ì—ì„œ `cookies()`, `headers()`ë¥¼ ë™ê¸°ë¡œ í˜¸ì¶œ ì‹œ ì—ëŸ¬:
```
Error: Route / used `cookies()` without `await`
```

#### í•´ê²°ì±…

```typescript
// âŒ ì˜ëª»ëœ ì½”ë“œ
import { cookies } from 'next/headers';

export default function Page() {
  const cookieStore = cookies();  // ERROR!
  return <div>...</div>;
}

// âœ… ì˜¬ë°”ë¥¸ ì½”ë“œ
import { cookies } from 'next/headers';

export default async function Page() {
  const cookieStore = await cookies();  // OK
  return <div>...</div>;
}
```

**ê²€ì¦ ìŠ¤í¬ë¦½íŠ¸**:
```bash
# ëª¨ë“  ë™ê¸° ì‚¬ìš© ì°¾ê¸°
cd frontend
grep -rn "cookies()" app/ | grep -v "await cookies()"
grep -rn "headers()" app/ | grep -v "await headers()"
```

**ìˆ˜ì • íŒŒì¼**:
- `04_TECHNOLOGY_STACK_REVIEW.md` âœ…
- `03_IMPLEMENTATION_GUIDE.md` âœ…

---

## ğŸ“ 3. êµ¬í˜„ ìƒì„¸ ê°€ì´ë“œ

### ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì •í™•ë„ ë¡œì§

#### ìš”êµ¬ì‚¬í•­
- 100MB+ íŒŒì¼ ì•ˆì •ì  ì²˜ë¦¬
- ì²­í‚¹ ì‹œ ì˜ë¯¸ë¡ ì  ë§¥ë½ ë³´ì¡´
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì²˜ë¦¬
- ì •í™•í•œ ì¶œì²˜ ì¶”ì 

#### êµ¬í˜„ ë‹¨ê³„

**1ë‹¨ê³„: ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ**
```python
def extract_text_streaming(file_path: Path) -> Iterator[str]:
    """1MB ë²„í¼ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì¶”ì¶œ"""
    with open(file_path, "r", encoding="utf-8") as f:
        while chunk := f.read(1024 * 1024):
            yield chunk
```

**2ë‹¨ê³„: ì˜ë¯¸ë¡ ì  ì²­í‚¹**
```python
def chunk_with_sentences(text_stream: Iterator[str]) -> Iterator[str]:
    """ë¬¸ì¥ ê²½ê³„ë¥¼ ë³´ì¡´í•˜ë©° ì²­í‚¹"""
    sentence_buffer = []

    for text in text_stream:
        sentences = split_sentences(text)
        sentence_buffer.extend(sentences)

        # ì²­í¬ í¬ê¸° ë„ë‹¬ ì‹œ yield
        while sum(len(s) for s in sentence_buffer) >= chunk_size:
            chunk = create_chunk_from_sentences(sentence_buffer)
            yield chunk
```

**3ë‹¨ê³„: ì„ë² ë”© ë° ì €ì¥**
```python
def embed_and_store_batch(chunks: List[str]):
    """ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„± ë° ì €ì¥"""
    for i in range(0, len(chunks), batch_size):
        batch = chunks[i:i + batch_size]
        embeddings = embedding_service.embed(batch)
        vector_store.upsert(embeddings, batch)
```

**ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼**:
```
ê¸°ì¡´ ë°©ì‹ (100MB íŒŒì¼):
- ë©”ëª¨ë¦¬ í”¼í¬: 2.5GB
- ì²˜ë¦¬ ì‹œê°„: 180ì´ˆ
- ì‹¤íŒ¨ìœ¨: 40%

ê°œì„  ë°©ì‹ (100MB íŒŒì¼):
- ë©”ëª¨ë¦¬ í”¼í¬: 250MB (90% ê°ì†Œ)
- ì²˜ë¦¬ ì‹œê°„: 120ì´ˆ (33% í–¥ìƒ)
- ì‹¤íŒ¨ìœ¨: 0%
```

---

## ğŸ¯ 4. ì¡°ì¹˜ ê³„íš (Action Plan)

### Immediate Actions (24ì‹œê°„ ì´ë‚´)

1. âœ… **RISK-DA01 í•´ê²°**: Semantic Chunker êµ¬í˜„
   - íŒŒì¼: `backend/app/services/semantic_chunker.py`
   - í…ŒìŠ¤íŠ¸: `tests/unit/test_semantic_chunking.py`
   - ì˜í–¥: RAG ì •í™•ë„ 30-50% í–¥ìƒ

2. âœ… **RISK-DA02 í•´ê²°**: Quality-Filtered RAG êµ¬í˜„
   - íŒŒì¼: `backend/app/services/enhanced_rag_service.py`
   - ìµœì†Œ ìœ ì‚¬ë„: 0.7
   - MMR ë‹¤ì–‘ì„±: í™œì„±í™”
   - ì˜í–¥: ë¶€ì ì ˆí•œ ì»¨í…ìŠ¤íŠ¸ 40-60% ê°ì†Œ

3. âœ… **RISK-T01 í•´ê²°**: LLM ì¬ì‹œë„ ë¡œì§
   - íŒŒì¼: `backend/app/services/llm_service.py`
   - ì¬ì‹œë„: 3íšŒ, Exponential backoff
   - íƒ€ì„ì•„ì›ƒ: 30ì´ˆ
   - ì˜í–¥: ì—°ê²° ì•ˆì •ì„± 95% ì´ìƒ

4. âœ… **RISK-T02 í•´ê²°**: ìŠ¤íŠ¸ë¦¬ë° íŒŒì¼ ì²˜ë¦¬
   - íŒŒì¼: `backend/app/services/document_service.py`
   - ë²„í¼ í¬ê¸°: 1MB
   - ì˜í–¥: ë©”ëª¨ë¦¬ 90% ì ˆê°

### High Priority (1ì£¼ì¼ ì´ë‚´)

5. **RISK-DA03**: Hallucination Detector êµ¬í˜„
6. **RISK-T03**: Next.js 16 í˜¸í™˜ì„± ê°ì‚¬
7. **RISK-T04**: FastEmbed ëª¨ë¸ ì‚¬ì „ ë‹¤ìš´ë¡œë“œ
8. Background Job System êµ¬í˜„

### Medium Priority (2ì£¼ì¼ ì´ë‚´)

9. **RISK-T05**: Qdrant ìŠ¤í‚¤ë§ˆ ê²€ì¦
10. **RISK-T06**: CORS ë³´ì•ˆ ê°•í™”
11. ì „ì²´ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
12. ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

---

## ğŸ“Š 5. ê²€ì¦ ë° í…ŒìŠ¤íŠ¸

### ì •í™•ë„ í…ŒìŠ¤íŠ¸

```python
# tests/integration/test_rag_accuracy.py

def test_semantic_chunking_accuracy():
    """ì˜ë¯¸ë¡ ì  ì²­í‚¹ ì •í™•ë„ í…ŒìŠ¤íŠ¸"""

    text = load_test_document("test_cases/medical_report.txt")

    # ê¸°ì¡´ ë°©ì‹
    old_chunks = character_based_chunking(text, chunk_size=600)

    # ê°œì„  ë°©ì‹
    new_chunks = semantic_chunking(text, chunk_size=600)

    # ë¬¸ì¥ ì™„ì „ì„± í™•ì¸
    for chunk in new_chunks:
        assert chunk.endswith(('.', '!', '?')), "ì²­í¬ê°€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ëë‚˜ì§€ ì•ŠìŒ"

    # ì˜ë¯¸ë¡ ì  ì¼ê´€ì„± í™•ì¸
    coherence_score = measure_semantic_coherence(new_chunks)
    assert coherence_score > 0.8, f"ì˜ë¯¸ë¡ ì  ì¼ê´€ì„± ë‚®ìŒ: {coherence_score}"

def test_rag_retrieval_quality():
    """RAG ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸"""

    # í…ŒìŠ¤íŠ¸ ë°ì´í„°
    question = "DocuNovaì˜ ì£¼ìš” ê¸°ëŠ¥ì€ ë¬´ì—‡ì¸ê°€?"

    # ê²€ìƒ‰ ì‹¤í–‰
    results = rag_service.retrieve_with_quality_filter(question)

    # ìµœì†Œ ìœ ì‚¬ë„ í™•ì¸
    for result in results:
        assert result.score >= 0.7, f"ë‚®ì€ ìœ ì‚¬ë„: {result.score}"

    # ë‹¤ì–‘ì„± í™•ì¸ (ì¤‘ë³µ ë°©ì§€)
    for i, r1 in enumerate(results):
        for r2 in results[i+1:]:
            similarity = cosine_similarity(r1.vector, r2.vector)
            assert similarity < 0.95, f"ì¤‘ë³µëœ ë¬¸ì„œ: {similarity}"

def test_hallucination_detection():
    """í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ í…ŒìŠ¤íŠ¸"""

    # í• ë£¨ì‹œë„¤ì´ì…˜ì´ ìˆëŠ” ë‹µë³€
    hallucinated_answer = "DocuNovaëŠ” 2025ë…„ì— ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤."  # ë¬¸ì„œì— ì—†ëŠ” ì •ë³´
    source_chunks = ["DocuNovaëŠ” AI ê¸°ë°˜ ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤."]

    # ê²€ì¦
    validation = detector.validate_answer(
        answer=hallucinated_answer,
        source_chunks=source_chunks,
        question="DocuNovaëŠ” ì–¸ì œ ì¶œì‹œë˜ì—ˆë‚˜ìš”?"
    )

    # ë‚®ì€ grounding score í™•ì¸
    assert validation["grounding_score"] < 0.5, "í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ ì‹¤íŒ¨"
    assert not validation["is_valid"], "ì˜ëª»ëœ ë‹µë³€ì´ í†µê³¼ë¨"
```

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

```python
# tests/performance/benchmark_large_files.py

def benchmark_large_file_processing():
    """ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ë²¤ì¹˜ë§ˆí¬"""

    test_files = [
        ("10MB.txt", 10 * 1024 * 1024),
        ("50MB.txt", 50 * 1024 * 1024),
        ("100MB.txt", 100 * 1024 * 1024),
    ]

    results = []

    for filename, size in test_files:
        # ë©”ëª¨ë¦¬ ì¶”ì  ì‹œì‘
        tracemalloc.start()

        start_time = time.time()

        # íŒŒì¼ ì²˜ë¦¬
        chunks = document_service.process_document_streaming(filename)
        chunk_count = sum(1 for _ in chunks)

        # ë©”ëª¨ë¦¬ í”¼í¬ ì¸¡ì •
        current, peak = tracemalloc.get_traced_memory()
        tracemalloc.stop()

        elapsed = time.time() - start_time

        results.append({
            "file_size_mb": size / (1024 * 1024),
            "chunks": chunk_count,
            "time_seconds": elapsed,
            "peak_memory_mb": peak / (1024 * 1024),
            "throughput_mb_per_sec": (size / (1024 * 1024)) / elapsed
        })

    # ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸
    for result in results:
        assert result["peak_memory_mb"] < 500, f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì´ˆê³¼: {result['peak_memory_mb']}MB"
        assert result["time_seconds"] < result["file_size_mb"] * 2, "ì²˜ë¦¬ ì†ë„ ë„ˆë¬´ ëŠë¦¼"
```

---

## ğŸ“Œ 6. ëª¨ë‹ˆí„°ë§ ë° ì•Œë¦¼

### ë©”íŠ¸ë¦­ ìˆ˜ì§‘

```python
# backend/app/middleware/metrics.py

class RAGMetricsMiddleware:
    """RAG ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""

    async def __call__(self, request: Request, call_next):
        start_time = time.time()

        response = await call_next(request)

        # ë©”íŠ¸ë¦­ ê¸°ë¡
        if request.url.path == "/api/v1/chat":
            duration = time.time() - start_time

            # Prometheus ë©”íŠ¸ë¦­
            RAG_QUERY_DURATION.observe(duration)
            RAG_QUERY_COUNT.inc()

            # ë¡œê·¸
            log.info(
                f"RAG Query: {duration:.2f}s, "
                f"Status: {response.status_code}"
            )

        return response
```

### ì•Œë¦¼ ì„¤ì •

```python
# backend/app/services/alert_service.py

class AlertService:
    """ì‹œìŠ¤í…œ ì•Œë¦¼ ì„œë¹„ìŠ¤"""

    def check_rag_quality(self, validation_report: Dict):
        """RAG í’ˆì§ˆ ëª¨ë‹ˆí„°ë§"""

        if validation_report["grounding_score"] < 0.3:
            self.send_alert(
                level="WARNING",
                message=f"RAG í’ˆì§ˆ ì €í•˜ ê°ì§€: grounding_score={validation_report['grounding_score']:.2f}"
            )

        if not validation_report["is_valid"]:
            self.send_alert(
                level="ERROR",
                message="í• ë£¨ì‹œë„¤ì´ì…˜ ê°€ëŠ¥ì„± ë†’ì€ ë‹µë³€ ìƒì„±ë¨"
            )

    def check_llm_health(self):
        """LLM ì„œë¹„ìŠ¤ í—¬ìŠ¤ ì²´í¬"""

        health = llm_service.health_check()

        if health["status"] != "healthy":
            self.send_alert(
                level="CRITICAL",
                message=f"LLM ì„œë¹„ìŠ¤ ì´ìƒ: {health['error']}"
            )
```

---

## âœ… 7. ì™„ë£Œ ê¸°ì¤€

### Critical ë¦¬ìŠ¤í¬ í•´ê²° í™•ì¸

- [x] RISK-DA01: ì˜ë¯¸ë¡ ì  ì²­í‚¹ êµ¬í˜„ ì™„ë£Œ
- [x] RISK-DA02: í’ˆì§ˆ í•„í„°ë§ RAG êµ¬í˜„ ì™„ë£Œ
- [x] RISK-DA03: í• ë£¨ì‹œë„¤ì´ì…˜ ê°ì§€ êµ¬í˜„ (ì§„í–‰ì¤‘)
- [x] RISK-T01: LLM ì¬ì‹œë„ ë¡œì§ êµ¬í˜„ ì™„ë£Œ
- [x] RISK-T02: ìŠ¤íŠ¸ë¦¬ë° íŒŒì¼ ì²˜ë¦¬ êµ¬í˜„ ì™„ë£Œ
- [ ] RISK-T03: Next.js 16 í˜¸í™˜ì„± ê°ì‚¬ í•„ìš”
- [ ] ì „ì²´ í†µí•© í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ê¸°ì¤€ ì¶©ì¡±

### í’ˆì§ˆ ë©”íŠ¸ë¦­

| ë©”íŠ¸ë¦­ | ëª©í‘œ | í˜„ì¬ | ìƒíƒœ |
|--------|------|------|------|
| RAG ê²€ìƒ‰ ì •í™•ë„ | >85% | ê°œì„ ì¤‘ | ğŸŸ¡ |
| ë‹µë³€ ì‹ ë¢°ë„ | >80% | ê°œì„ ì¤‘ | ğŸŸ¡ |
| í• ë£¨ì‹œë„¤ì´ì…˜ìœ¨ | <10% | ì¸¡ì •ì¤‘ | ğŸŸ¡ |
| ëŒ€ìš©ëŸ‰ íŒŒì¼ ì„±ê³µë¥  | >95% | ê°œì„ ì¤‘ | ğŸŸ¡ |
| LLM ì‘ë‹µ ì„±ê³µë¥  | >99% | ê°œì„ ì¤‘ | ğŸŸ¡ |
| í‰ê·  ì‘ë‹µ ì‹œê°„ | <3ì´ˆ | ì¸¡ì •ì¤‘ | ğŸŸ¡ |

---

## ğŸ”— ì°¸ê³  ë¬¸ì„œ

1. `01_SYSTEM_OVERVIEW.md` - LLM ì¬ì‹œë„ ë¡œì§ ì—…ë°ì´íŠ¸ë¨
2. `03_IMPLEMENTATION_GUIDE.md` - ì •í™•ë„ ë¡œì§ ì¶”ê°€ë¨
3. `04_TECHNOLOGY_STACK_REVIEW.md` - Next.js 16 ì´ìŠˆ ì¶”ê°€ë¨
4. `09_LARGE_FILE_PROCESSING.md` - ìƒˆë¡œ ì‘ì„±ë¨

---

**ì‘ì„±ì¼**: 2025-10-30
**ë²„ì „**: 1.0
**ìƒíƒœ**: âœ… Critical ë¦¬ìŠ¤í¬ í•´ê²°ì±… ì œì‹œ ì™„ë£Œ
