"""
DocuNova Backend - Main Application Entry Point
FastAPI ê¸°ë°˜ RAG ì‹œìŠ¤í…œ
"""

from fastapi import FastAPI, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
import logging
import os
from contextlib import asynccontextmanager

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
log = logging.getLogger(__name__)

# Global services (initialized in lifespan)
embedding_service = None
vector_service = None
llm_service = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘/ì¢…ë£Œ ì‹œ ì‹¤í–‰ë˜ëŠ” lifespan ì´ë²¤íŠ¸"""

    # Startup
    log.info("ğŸš€ DocuNova Backend ì‹œì‘ ì¤‘...")

    try:
        # ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        from app.services.embedding_service import EmbeddingService
        from app.services.vector_service import VectorService
        from app.services.llm_service import LLMService

        global embedding_service, vector_service, llm_service

        # Embedding ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        log.info("Embedding ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        embedding_service = EmbeddingService(
            model_name=os.getenv("EMBEDDING_MODEL", "BAAI/bge-small-en-v1.5"),
            cache_dir="./embedding_models"
        )
        await embedding_service.initialize()
        log.info("âœ… Embedding ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

        # Vector ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        log.info("Vector ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        vector_service = VectorService(
            host=os.getenv("QDRANT_HOST", "localhost"),
            port=int(os.getenv("QDRANT_PORT", "6333")),
            collection_name=os.getenv("COLLECTION_NAME", "docunova_documents"),
            vector_size=384  # bge-small-en-v1.5 dimension
        )
        vector_service.initialize_collection()
        log.info("âœ… Vector ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

        # LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™”
        log.info("LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì¤‘...")
        llm_service = LLMService(
            host=os.getenv("OLLAMA_HOST", "localhost"),
            port=int(os.getenv("OLLAMA_PORT", "11434")),
            model=os.getenv("OLLAMA_MODEL", "llama3.1:8b")
        )
        health = await llm_service.health_check()
        if health["status"] == "healthy":
            log.info("âœ… LLM ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")
        else:
            log.warning(f"âš ï¸ LLM ì„œë¹„ìŠ¤ ìƒíƒœ ë¶ˆì•ˆì •: {health}")

        log.info("âœ… ëª¨ë“  ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ")

    except Exception as e:
        log.error(f"âŒ ì„œë¹„ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}", exc_info=True)
        raise

    yield  # ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹¤í–‰

    # Shutdown
    log.info("ğŸ›‘ DocuNova Backend ì¢…ë£Œ ì¤‘...")

# FastAPI ì•± ìƒì„±
app = FastAPI(
    title="DocuNova API",
    description="AI-Powered Document Analysis & Q&A System",
    version="1.0.0",
    lifespan=lifespan
)

# CORS ì„¤ì •
ENVIRONMENT = os.getenv("ENVIRONMENT", "development")

if ENVIRONMENT == "production":
    allowed_origins = os.getenv("CORS_ORIGINS", "").split(",")
else:
    allowed_origins = ["http://localhost:3000", "http://127.0.0.1:3000"]

app.add_middleware(
    CORSMiddleware,
    allow_origins=allowed_origins,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["Content-Type", "Authorization", "X-Request-ID"],
    max_age=600
)

# Global exception handler
@app.exception_handler(Exception)
async def global_exception_handler(request: Request, exc: Exception):
    log.error(f"Unhandled exception: {exc}", exc_info=True)
    return JSONResponse(
        status_code=500,
        content={
            "error": "Internal Server Error",
            "message": "ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
            "detail": str(exc) if ENVIRONMENT == "development" else None
        }
    )

# Health check endpoint
@app.get("/health")
async def health_check():
    """ì‹œìŠ¤í…œ ì „ì²´ í—¬ìŠ¤ ì²´í¬"""

    health_status = {
        "status": "healthy",
        "services": {}
    }

    # Embedding service
    if embedding_service:
        emb_health = embedding_service.health_check()
        health_status["services"]["embedding"] = emb_health
        if emb_health["status"] != "healthy":
            health_status["status"] = "degraded"

    # Vector service
    if vector_service:
        vec_health = vector_service.health_check()
        health_status["services"]["vector"] = vec_health
        if vec_health["status"] != "healthy":
            health_status["status"] = "degraded"

    # LLM service
    if llm_service:
        llm_health = await llm_service.health_check()
        health_status["services"]["llm"] = llm_health
        if llm_health["status"] != "healthy":
            health_status["status"] = "degraded"

    return health_status

# Root endpoint
@app.get("/")
async def root():
    return {
        "message": "DocuNova API",
        "version": "1.0.0",
        "status": "running",
        "docs": "/docs",
        "health": "/health"
    }

# API ë¼ìš°í„° ë“±ë¡
from app.api.v1 import chat, documents, collections

app.include_router(chat.router, prefix="/api/v1", tags=["chat"])
app.include_router(documents.router, prefix="/api/v1", tags=["documents"])
app.include_router(collections.router, prefix="/api/v1", tags=["collections"])

if __name__ == "__main__":
    import uvicorn

    port = int(os.getenv("PORT", "8000"))

    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=port,
        reload=ENVIRONMENT == "development",
        log_level="info"
    )
