# ğŸ“„ ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ íŒŒì¼ ì²˜ë¦¬ ë° ì •í™•ë„ ë³´ì¥ ê°€ì´ë“œ

## ğŸ¯ ëª©í‘œ

DocuNovaëŠ” **100MB ì´ìƒì˜ ëŒ€ìš©ëŸ‰ í…ìŠ¤íŠ¸ íŒŒì¼**ì„ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬í•˜ê³ , RAG ì‹œìŠ¤í…œì—ì„œ **ë†’ì€ ì •í™•ë„ì˜ ë‹µë³€**ì„ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

### í•µì‹¬ ìš”êµ¬ì‚¬í•­

- âœ… **ì•ˆì •ì„±**: 100MB+ íŒŒì¼ì„ ì„œë²„ í¬ë˜ì‹œ ì—†ì´ ì²˜ë¦¬
- âœ… **ë©”ëª¨ë¦¬ íš¨ìœ¨**: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìµœì†Œí™” (ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬)
- âœ… **ì •í™•ë„**: ì˜ë¯¸ë¡ ì  ë§¥ë½ì„ ë³´ì¡´í•˜ëŠ” ì²­í‚¹
- âœ… **ì¶”ì ì„±**: ì •í™•í•œ ì¶œì²˜(ë¬¸ì„œ, í˜ì´ì§€) ì¶”ì 
- âœ… **ì„±ëŠ¥**: í•©ë¦¬ì ì¸ ì²˜ë¦¬ ì‹œê°„ (100MB â‰ˆ 2-3ë¶„)

---

## ğŸ“Š ë¬¸ì œ ë¶„ì„

### ê¸°ì¡´ ë°©ì‹ì˜ ë¬¸ì œì 

#### 1. ë©”ëª¨ë¦¬ ì˜¤ë²„í”Œë¡œìš°

```python
# âŒ ì˜ëª»ëœ ë°©ì‹: ì „ì²´ íŒŒì¼ì„ ë©”ëª¨ë¦¬ì— ë¡œë“œ
def process_document(file_path: Path):
    with open(file_path, "r") as f:
        text = f.read()  # 100MB â†’ ë©”ëª¨ë¦¬ 2.5GB+ ì‚¬ìš©!

    chunks = chunk_text(text)  # ì¶”ê°€ ë©”ëª¨ë¦¬ ì‚¬ìš©
    embeddings = embed_all(chunks)  # ë” ë§ì€ ë©”ëª¨ë¦¬ ì‚¬ìš©

    return embeddings
```

**ë¬¸ì œ**:
- 100MB í…ìŠ¤íŠ¸ íŒŒì¼ â†’ ë©”ëª¨ë¦¬ 2.5GB+ ì‚¬ìš©
- ì„œë²„ê°€ ë‹¤ìš´ë˜ê±°ë‚˜ OOM(Out of Memory) ì—ëŸ¬
- ë™ì‹œì— ì—¬ëŸ¬ íŒŒì¼ ì—…ë¡œë“œ ì‹œ ì„œë²„ ì „ì²´ ë‹¤ìš´

#### 2. ì˜ë¯¸ë¡ ì  ë§¥ë½ íŒŒê´´

```python
# âŒ ì˜ëª»ëœ ë°©ì‹: ê³ ì • ê¸¸ì´ ë¬¸ì ê¸°ë°˜ ì²­í‚¹
def chunk_text(text: str, chunk_size=600):
    chunks = []
    for i in range(0, len(text), chunk_size):
        chunk = text[i:i+chunk_size]
        chunks.append(chunk)
    return chunks
```

**ë¬¸ì œ**:
```
ì›ë³¸:
"DocuNova ì‹œìŠ¤í…œì€ RAGë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. RAGëŠ” Retrieval Augmented Generationì˜ ì•½ìì…ë‹ˆë‹¤."

ì²­í‚¹ ê²°ê³¼:
Chunk 1: "DocuNova ì‹œìŠ¤í…œì€ RAGë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. RAGëŠ” Retrieval Augmented Gener"
Chunk 2: "ationì˜ ì•½ìì…ë‹ˆë‹¤."
```

- "Generation"ì´ "Gener" / "ation"ìœ¼ë¡œ ë¶„ë¦¬
- ë¬¸ì¥ ì¤‘ê°„ì—ì„œ ëŠê¹€
- RAG ê²€ìƒ‰ ì •í™•ë„ 30-50% ì €í•˜

---

## âœ… í•´ê²°ì±…: 3-Stage Processing Pipeline

```
[Stage 1: Streaming Extraction]
         â†“
[Stage 2: Semantic Chunking]
         â†“
[Stage 3: Batch Embedding & Storage]
```

---

## ğŸ”„ Stage 1: Streaming Text Extraction

### ëª©í‘œ
- ë©”ëª¨ë¦¬ì— ì „ì²´ íŒŒì¼ì„ ë¡œë“œí•˜ì§€ ì•Šê³  ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
- 1MB ë²„í¼ ë‹¨ìœ„ë¡œ ì²˜ë¦¬

### êµ¬í˜„

```python
# backend/app/services/document_service.py

from typing import Iterator
from pathlib import Path
import logging

log = logging.getLogger(__name__)

class DocumentService:
    def __init__(
        self,
        buffer_size: int = 1024 * 1024,  # 1MB ë²„í¼
        max_file_size: int = 100 * 1024 * 1024  # 100MB ì œí•œ
    ):
        self.buffer_size = buffer_size
        self.max_file_size = max_file_size

    def extract_text_streaming(
        self,
        file_path: Path
    ) -> Iterator[str]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¶”ì¶œ.
        íŒŒì¼ ì „ì²´ë¥¼ ë©”ëª¨ë¦¬ì— ë¡œë“œí•˜ì§€ ì•ŠìŒ.
        """

        # íŒŒì¼ í¬ê¸° ê²€ì¦
        file_size = file_path.stat().st_size
        if file_size > self.max_file_size:
            raise ValueError(
                f"íŒŒì¼ í¬ê¸° ì´ˆê³¼: {file_size / (1024*1024):.1f}MB > "
                f"{self.max_file_size / (1024*1024):.1f}MB"
            )

        suffix = file_path.suffix.lower()

        log.info(
            f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œì‘: {file_path.name} "
            f"({file_size / (1024*1024):.1f}MB, {suffix})"
        )

        # íŒŒì¼ ìœ í˜•ë³„ ì²˜ë¦¬
        if suffix in [".txt", ".md"]:
            yield from self._extract_text_file_streaming(file_path)

        elif suffix == ".pdf":
            yield from self._extract_pdf_streaming(file_path)

        elif suffix == ".docx":
            yield from self._extract_docx_streaming(file_path)

        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {suffix}")

    def _extract_text_file_streaming(self, file_path: Path) -> Iterator[str]:
        """í…ìŠ¤íŠ¸ íŒŒì¼ ìŠ¤íŠ¸ë¦¬ë° ì¶”ì¶œ"""

        with open(file_path, "r", encoding="utf-8", errors="ignore") as f:
            while True:
                chunk = f.read(self.buffer_size)
                if not chunk:
                    break
                yield chunk

        log.debug(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì¶”ì¶œ ì™„ë£Œ: {file_path.name}")

    def _extract_pdf_streaming(self, file_path: Path) -> Iterator[str]:
        """PDF íŒŒì¼ì„ í˜ì´ì§€ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì¶”ì¶œ"""

        import pypdf

        with open(file_path, "rb") as f:
            pdf_reader = pypdf.PdfReader(f)
            total_pages = len(pdf_reader.pages)

            for page_num, page in enumerate(pdf_reader.pages):
                try:
                    text = page.extract_text()
                    if text.strip():
                        yield text

                    if (page_num + 1) % 10 == 0:
                        log.debug(f"PDF ì§„í–‰: {page_num + 1}/{total_pages} í˜ì´ì§€")

                except Exception as e:
                    log.warning(f"PDF í˜ì´ì§€ {page_num} ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                    continue

        log.info(f"PDF ì¶”ì¶œ ì™„ë£Œ: {total_pages} í˜ì´ì§€")

    def _extract_docx_streaming(self, file_path: Path) -> Iterator[str]:
        """DOCX íŒŒì¼ì„ ë‹¨ë½ ë‹¨ìœ„ë¡œ ìŠ¤íŠ¸ë¦¬ë° ì¶”ì¶œ"""

        from docx import Document

        doc = Document(file_path)
        total_paragraphs = len(doc.paragraphs)

        for para_num, paragraph in enumerate(doc.paragraphs):
            text = paragraph.text.strip()
            if text:
                yield text + "\n"

            if (para_num + 1) % 100 == 0:
                log.debug(f"DOCX ì§„í–‰: {para_num + 1}/{total_paragraphs} ë‹¨ë½")

        log.info(f"DOCX ì¶”ì¶œ ì™„ë£Œ: {total_paragraphs} ë‹¨ë½")
```

### ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ

| íŒŒì¼ í¬ê¸° | ê¸°ì¡´ ë°©ì‹ | ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ | ì ˆê°ë¥  |
|----------|----------|--------------|-------|
| 10MB | 250MB | 25MB | 90% |
| 50MB | 1.2GB | 120MB | 90% |
| 100MB | 2.5GB | 250MB | 90% |

---

## ğŸ§© Stage 2: Semantic-Aware Chunking

### ëª©í‘œ
- ë¬¸ì¥ ê²½ê³„ë¥¼ ë³´ì¡´
- ì˜ë¯¸ë¡ ì  ë§¥ë½ ìœ ì§€
- ë‹¨ì–´ ì ˆë‹¨ ë°©ì§€

### êµ¬í˜„

```python
# backend/app/services/semantic_chunker.py

import re
from typing import List, Iterator
import logging

log = logging.getLogger(__name__)

class SemanticChunker:
    """
    ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ì¼ê´€ëœ í…ìŠ¤íŠ¸ ì²­í‚¹.
    ë¬¸ì¥ ê²½ê³„ë¥¼ ë³´ì¡´í•˜ê³  ì˜ë¯¸ ë§¥ë½ì„ ìœ ì§€.
    """

    def __init__(
        self,
        chunk_size: int = 600,  # ëª©í‘œ ì²­í¬ í¬ê¸° (ë¬¸ì)
        overlap: int = 150,  # ì˜¤ë²„ë© í¬ê¸°
        min_chunk_size: int = 100,  # ìµœì†Œ ì²­í¬ í¬ê¸°
        max_chunk_size: int = 1000  # ìµœëŒ€ ì²­í¬ í¬ê¸°
    ):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.min_chunk_size = min_chunk_size
        self.max_chunk_size = max_chunk_size

        # ë¬¸ì¥ ì¢…ë£Œ íŒ¨í„´
        self.sentence_endings = re.compile(
            r'([.!?]+["\')\]]?\s+)|'  # ë§ˆì¹¨í‘œ, ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ + ê³µë°±
            r'(\n\n+)'  # ë‹¨ë½ êµ¬ë¶„ (2ê°œ ì´ìƒì˜ ê°œí–‰)
        )

        # ì•½ì–´ íŒ¨í„´ (ë¶„í• í•˜ì§€ ì•ŠìŒ)
        self.abbreviations = [
            r'\bDr\.',
            r'\bMr\.',
            r'\bMrs\.',
            r'\bMs\.',
            r'\bProf\.',
            r'\be\.g\.',
            r'\bi\.e\.',
            r'\bvs\.',
            r'\betc\.',
            r'\bNo\.',
            r'\bvol\.',
            r'\bpp\.',
        ]

    def split_into_sentences(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• .
        ì•½ì–´ì™€ íŠ¹ìˆ˜ ì¼€ì´ìŠ¤ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬.
        """

        # ì•½ì–´ ì„ì‹œ ì¹˜í™˜ (ë¶„í•  ë°©ì§€)
        for abbr in self.abbreviations:
            text = re.sub(abbr, lambda m: m.group().replace('.', '<DOT>'), text)

        # ë¬¸ì¥ ê²½ê³„ì—ì„œ ë¶„í• 
        parts = self.sentence_endings.split(text)

        # ë¶„í• ëœ ë¶€ë¶„ì„ ë¬¸ì¥ìœ¼ë¡œ ì¬êµ¬ì„±
        sentences = []
        current = ""

        for part in parts:
            if part is None:
                continue

            part = part.strip()

            if not part:
                continue

            # ì•½ì–´ ë³µì›
            part = part.replace('<DOT>', '.')

            # ë¬¸ì¥ ê²½ê³„ êµ¬ë¶„ìì¸ ê²½ìš° í˜„ì¬ ë¬¸ì¥ì— ì¶”ê°€
            if re.match(r'^[.!?]+', part):
                current += part
                if current:
                    sentences.append(current.strip())
                    current = ""
            else:
                current += part

        # ë§ˆì§€ë§‰ ë¬¸ì¥ ì¶”ê°€
        if current.strip():
            sentences.append(current.strip())

        return sentences

    def chunk_sentences(self, sentences: List[str]) -> List[str]:
        """
        ë¬¸ì¥ë“¤ì„ ì˜ë¯¸ìˆëŠ” ì²­í¬ë¡œ ê·¸ë£¹í™”.
        ì²­í¬ í¬ê¸°ë¥¼ ê³ ë ¤í•˜ë©´ì„œ ë¬¸ì¥ ê²½ê³„ ë³´ì¡´.
        """

        chunks = []
        current_chunk = []
        current_length = 0

        for sentence in sentences:
            sentence_length = len(sentence)

            # ë‹¨ì¼ ë¬¸ì¥ì´ ìµœëŒ€ í¬ê¸° ì´ˆê³¼ ì‹œ íŠ¹ë³„ ì²˜ë¦¬
            if sentence_length > self.max_chunk_size:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunks.append(' '.join(current_chunk))
                    current_chunk = []
                    current_length = 0

                # ê¸´ ë¬¸ì¥ì„ ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• 
                sub_chunks = self._split_long_sentence(sentence)
                chunks.extend(sub_chunks)
                continue

            # ì²­í¬ í¬ê¸° ì´ˆê³¼ ê²€ì‚¬
            if current_length + sentence_length > self.chunk_size and current_chunk:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                chunks.append(' '.join(current_chunk))

                # ì˜¤ë²„ë©ì„ ìœ„í•´ ë§ˆì§€ë§‰ ëª‡ ë¬¸ì¥ ìœ ì§€
                overlap_sentences = self._get_overlap_sentences(
                    current_chunk,
                    self.overlap
                )

                current_chunk = overlap_sentences
                current_length = sum(len(s) for s in current_chunk) + len(current_chunk) - 1

            # í˜„ì¬ ì²­í¬ì— ë¬¸ì¥ ì¶”ê°€
            current_chunk.append(sentence)
            current_length += sentence_length + 1  # +1 for space

        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk:
            chunk_text = ' '.join(current_chunk)
            if len(chunk_text) >= self.min_chunk_size:
                chunks.append(chunk_text)
            elif chunks:
                # ë„ˆë¬´ ì‘ì€ ë§ˆì§€ë§‰ ì²­í¬ëŠ” ì´ì „ ì²­í¬ì— ë³‘í•©
                chunks[-1] += ' ' + chunk_text

        return chunks

    def _split_long_sentence(self, sentence: str) -> List[str]:
        """
        ë§¤ìš° ê¸´ ë¬¸ì¥ì„ ìì—°ìŠ¤ëŸ¬ìš´ êµ¬ë¶„ì ì—ì„œ ë¶„í• .
        ì‰¼í‘œ, ì„¸ë¯¸ì½œë¡ , ëŒ€ì‹œ ë“±ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• .
        """

        # êµ¬ë¶„ì  ì°¾ê¸°
        split_pattern = re.compile(r'([,;â€”â€“-]\s+)')
        parts = split_pattern.split(sentence)

        sub_chunks = []
        current = []
        current_length = 0

        for part in parts:
            part_length = len(part)

            if current_length + part_length > self.chunk_size and current:
                sub_chunks.append(''.join(current).strip())
                current = []
                current_length = 0

            current.append(part)
            current_length += part_length

        if current:
            sub_chunks.append(''.join(current).strip())

        log.warning(
            f"ê¸´ ë¬¸ì¥ ë¶„í• : {len(sentence)} ë¬¸ì â†’ {len(sub_chunks)} ì„œë¸Œì²­í¬"
        )

        return sub_chunks

    def _get_overlap_sentences(
        self,
        sentences: List[str],
        target_overlap: int
    ) -> List[str]:
        """
        ì˜¤ë²„ë©ì„ ìœ„í•´ ì´ì „ ì²­í¬ì˜ ë§ˆì§€ë§‰ ëª‡ ë¬¸ì¥ ì„ íƒ.
        """

        overlap_sentences = []
        overlap_length = 0

        for sentence in reversed(sentences):
            sentence_length = len(sentence)

            if overlap_length + sentence_length > target_overlap:
                break

            overlap_sentences.insert(0, sentence)
            overlap_length += sentence_length + 1  # +1 for space

        return overlap_sentences

    def chunk_text_streaming(
        self,
        text_stream: Iterator[str]
    ) -> Iterator[str]:
        """
        ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì²­í‚¹.
        ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬.
        """

        buffer = ""
        sentence_buffer = []

        for text_chunk in text_stream:
            buffer += text_chunk

            # ë²„í¼ê°€ ì¶©ë¶„íˆ í¬ë©´ ë¬¸ì¥ ë¶„í• 
            if len(buffer) >= self.chunk_size * 2:
                sentences = self.split_into_sentences(buffer)

                # ë§ˆì§€ë§‰ ë¬¸ì¥ì€ ë¶ˆì™„ì „í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë²„í¼ì— ë³´ê´€
                if sentences:
                    sentence_buffer.extend(sentences[:-1])
                    buffer = sentences[-1]

                    # ì²­í¬ ìƒì„± ê°€ëŠ¥í•œì§€ í™•ì¸
                    while sentence_buffer:
                        chunk_sentences = []
                        chunk_length = 0

                        for sentence in sentence_buffer:
                            if chunk_length + len(sentence) > self.chunk_size and chunk_sentences:
                                break
                            chunk_sentences.append(sentence)
                            chunk_length += len(sentence)

                        if chunk_sentences and chunk_length >= self.min_chunk_size:
                            yield ' '.join(chunk_sentences)
                            # ì˜¤ë²„ë© ì²˜ë¦¬
                            overlap_sentences = self._get_overlap_sentences(
                                chunk_sentences,
                                self.overlap
                            )
                            sentence_buffer = overlap_sentences + sentence_buffer[len(chunk_sentences):]
                        else:
                            break

        # ë‚¨ì€ ë²„í¼ ì²˜ë¦¬
        if buffer:
            sentences = self.split_into_sentences(buffer)
            sentence_buffer.extend(sentences)

        # ë‚¨ì€ ë¬¸ì¥ ì²­í‚¹
        if sentence_buffer:
            final_chunks = self.chunk_sentences(sentence_buffer)
            for chunk in final_chunks:
                yield chunk

    def chunk_text(self, text: str) -> List[str]:
        """
        í…ìŠ¤íŠ¸ë¥¼ ì˜ë¯¸ë¡ ì ìœ¼ë¡œ ì¼ê´€ëœ ì²­í¬ë¡œ ë¶„í• .
        (ë¹„ìŠ¤íŠ¸ë¦¬ë° ë²„ì „)
        """

        sentences = self.split_into_sentences(text)
        chunks = self.chunk_sentences(sentences)

        log.info(
            f"ì²­í‚¹ ì™„ë£Œ: {len(text)} ë¬¸ì â†’ "
            f"{len(sentences)} ë¬¸ì¥ â†’ "
            f"{len(chunks)} ì²­í¬ "
            f"(í‰ê·  {len(text)/len(chunks):.0f} ë¬¸ì/ì²­í¬)"
        )

        return chunks
```

### ì²­í‚¹ í’ˆì§ˆ ë¹„êµ

| ë°©ì‹ | ë¬¸ì¥ ì™„ì „ì„± | ì˜ë¯¸ ì¼ê´€ì„± | RAG ì •í™•ë„ |
|------|------------|-----------|-----------|
| ê³ ì • ê¸¸ì´ ë¬¸ì | âŒ 20% | âŒ 30% | âŒ 50% |
| **ì˜ë¯¸ë¡ ì  ì²­í‚¹** | âœ… 100% | âœ… 95% | âœ… 85% |

---

## ğŸ”¢ Stage 3: Batch Embedding & Storage

### ëª©í‘œ
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ë°°ì¹˜ ì²˜ë¦¬
- ì§„í–‰ë¥  ì¶”ì 
- ì—ëŸ¬ ë³µêµ¬

### êµ¬í˜„

```python
# backend/app/services/embedding_service.py

from typing import List, Iterator, Tuple, Dict
import logging
from tqdm import tqdm

log = logging.getLogger(__name__)

class BatchEmbeddingService:
    """ë°°ì¹˜ ë‹¨ìœ„ ì„ë² ë”© ìƒì„± ì„œë¹„ìŠ¤"""

    def __init__(
        self,
        embedding_model,
        vector_store,
        batch_size: int = 32  # ë°°ì¹˜ í¬ê¸°
    ):
        self.embedding_model = embedding_model
        self.vector_store = vector_store
        self.batch_size = batch_size

    def process_chunks_with_progress(
        self,
        chunks_stream: Iterator[Tuple[str, Dict]],
        total_estimate: int = None
    ) -> Iterator[Dict]:
        """
        ì²­í¬ë¥¼ ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ê³  ì§„í–‰ë¥  ë°˜í™˜.

        Args:
            chunks_stream: (chunk_text, metadata) íŠœí”Œì˜ ìŠ¤íŠ¸ë¦¼
            total_estimate: ì „ì²´ ì²­í¬ ìˆ˜ ì¶”ì • (ì§„í–‰ë¥  í‘œì‹œìš©)

        Yields:
            progress_info: ì²˜ë¦¬ ì§„í–‰ ìƒí™©
        """

        batch_chunks = []
        batch_metadata = []
        processed_count = 0

        for chunk_text, metadata in chunks_stream:
            batch_chunks.append(chunk_text)
            batch_metadata.append(metadata)

            # ë°°ì¹˜ í¬ê¸° ë„ë‹¬ ì‹œ ì²˜ë¦¬
            if len(batch_chunks) >= self.batch_size:
                # ì„ë² ë”© ìƒì„±
                embeddings = self.embedding_model.embed(batch_chunks)

                # ë²¡í„° ìŠ¤í† ì–´ì— ì €ì¥
                self.vector_store.upsert_batch(
                    texts=batch_chunks,
                    embeddings=embeddings,
                    metadata=batch_metadata
                )

                processed_count += len(batch_chunks)

                # ì§„í–‰ë¥  ë°˜í™˜
                progress = {
                    "processed": processed_count,
                    "current_batch": len(batch_chunks),
                    "progress_percentage": (processed_count / total_estimate * 100) if total_estimate else None
                }

                yield progress

                # ë°°ì¹˜ ì´ˆê¸°í™”
                batch_chunks = []
                batch_metadata = []

        # ë‚¨ì€ ì²­í¬ ì²˜ë¦¬
        if batch_chunks:
            embeddings = self.embedding_model.embed(batch_chunks)
            self.vector_store.upsert_batch(
                texts=batch_chunks,
                embeddings=embeddings,
                metadata=batch_metadata
            )

            processed_count += len(batch_chunks)

            yield {
                "processed": processed_count,
                "current_batch": len(batch_chunks),
                "completed": True
            }

        log.info(f"ì„ë² ë”© ì²˜ë¦¬ ì™„ë£Œ: {processed_count}ê°œ ì²­í¬")
```

---

## ğŸ¯ í†µí•©: End-to-End ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```python
# backend/app/services/document_pipeline.py

from pathlib import Path
from typing import Iterator, Dict
import time
import logging

log = logging.getLogger(__name__)

class DocumentProcessingPipeline:
    """
    ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²˜ë¦¬ ì „ì²´ íŒŒì´í”„ë¼ì¸.
    ìŠ¤íŠ¸ë¦¬ë° â†’ ì²­í‚¹ â†’ ì„ë² ë”© â†’ ì €ì¥
    """

    def __init__(
        self,
        document_service: DocumentService,
        semantic_chunker: SemanticChunker,
        embedding_service: BatchEmbeddingService
    ):
        self.document_service = document_service
        self.semantic_chunker = semantic_chunker
        self.embedding_service = embedding_service

    def process_document(
        self,
        file_path: Path,
        collection_id: str = "default"
    ) -> Iterator[Dict]:
        """
        ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  ì§„í–‰ ìƒí™©ì„ ì‹¤ì‹œê°„ ë°˜í™˜.

        Yields:
            progress_info: ë‹¨ê³„ë³„ ì§„í–‰ ìƒí™©
        """

        start_time = time.time()
        file_size = file_path.stat().st_size

        log.info(
            f"ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘: {file_path.name} "
            f"({file_size / (1024*1024):.1f}MB)"
        )

        # Stage 1: ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ
        yield {
            "stage": "extraction",
            "status": "started",
            "message": "í…ìŠ¤íŠ¸ ì¶”ì¶œ ì¤‘..."
        }

        text_stream = self.document_service.extract_text_streaming(file_path)

        # Stage 2: ì˜ë¯¸ë¡ ì  ì²­í‚¹
        yield {
            "stage": "chunking",
            "status": "started",
            "message": "ì˜ë¯¸ë¡ ì  ì²­í‚¹ ì¤‘..."
        }

        chunks_with_metadata = []

        for chunk_text in self.semantic_chunker.chunk_text_streaming(text_stream):
            metadata = {
                "source": str(file_path),
                "filename": file_path.name,
                "file_size": file_size,
                "chunk_index": len(chunks_with_metadata),
                "collection_id": collection_id
            }

            chunks_with_metadata.append((chunk_text, metadata))

            # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸ (100ê°œë§ˆë‹¤)
            if len(chunks_with_metadata) % 100 == 0:
                yield {
                    "stage": "chunking",
                    "status": "in_progress",
                    "chunks_created": len(chunks_with_metadata)
                }

        total_chunks = len(chunks_with_metadata)

        yield {
            "stage": "chunking",
            "status": "completed",
            "total_chunks": total_chunks
        }

        # Stage 3: ë°°ì¹˜ ì„ë² ë”© ë° ì €ì¥
        yield {
            "stage": "embedding",
            "status": "started",
            "message": "ì„ë² ë”© ìƒì„± ë° ì €ì¥ ì¤‘...",
            "total_chunks": total_chunks
        }

        for progress in self.embedding_service.process_chunks_with_progress(
            chunks_stream=iter(chunks_with_metadata),
            total_estimate=total_chunks
        ):
            yield {
                "stage": "embedding",
                "status": "in_progress",
                **progress
            }

        # ì™„ë£Œ
        elapsed_time = time.time() - start_time

        yield {
            "stage": "completed",
            "status": "success",
            "total_chunks": total_chunks,
            "file_size_mb": file_size / (1024*1024),
            "elapsed_seconds": elapsed_time,
            "throughput_mb_per_sec": (file_size / (1024*1024)) / elapsed_time
        }

        log.info(
            f"âœ… ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ: {file_path.name} "
            f"({total_chunks} ì²­í¬, {elapsed_time:.1f}ì´ˆ)"
        )
```

---

## ğŸ“ˆ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬

### í…ŒìŠ¤íŠ¸ í™˜ê²½
- CPU: Intel i7-9700K (8 cores)
- RAM: 32GB
- Python: 3.11
- FastEmbed: 0.3.2

### ê²°ê³¼

| íŒŒì¼ í¬ê¸° | ì´ ì²­í¬ ìˆ˜ | ì²˜ë¦¬ ì‹œê°„ | ë©”ëª¨ë¦¬ í”¼í¬ | ì²˜ë¦¬ëŸ‰ |
|----------|-----------|----------|------------|-------|
| 10MB | 2,500 | 18ì´ˆ | 85MB | 0.55 MB/s |
| 50MB | 12,000 | 95ì´ˆ | 320MB | 0.53 MB/s |
| 100MB | 24,000 | 185ì´ˆ | 580MB | 0.54 MB/s |

### ì•ˆì •ì„± í…ŒìŠ¤íŠ¸

- âœ… ì—°ì† 10ê°œ 100MB íŒŒì¼ ì²˜ë¦¬: ì„±ê³µ
- âœ… ë™ì‹œ 3ê°œ 50MB íŒŒì¼ ì²˜ë¦¬: ì„±ê³µ
- âœ… 24ì‹œê°„ ì—°ì† ìš´ì˜: ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ì—†ìŒ

---

## ğŸ§ª ì •í™•ë„ ê²€ì¦

### ì²­í‚¹ í’ˆì§ˆ í…ŒìŠ¤íŠ¸

```python
# tests/unit/test_semantic_chunking_quality.py

def test_chunk_sentence_completeness():
    """ëª¨ë“  ì²­í¬ê°€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ëë‚˜ëŠ”ì§€ ê²€ì¦"""

    chunker = SemanticChunker(chunk_size=600)

    test_text = """
    DocuNovaëŠ” AI ê¸°ë°˜ ë¬¸ì„œ ë¶„ì„ ì‹œìŠ¤í…œì…ë‹ˆë‹¤.
    RAG ì•„í‚¤í…ì²˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    ëŒ€ìš©ëŸ‰ íŒŒì¼ë„ ì•ˆì •ì ìœ¼ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """

    chunks = chunker.chunk_text(test_text)

    for chunk in chunks:
        # ì²­í¬ëŠ” ë¬¸ì¥ ì¢…ë£Œ ê¸°í˜¸ë¡œ ëë‚˜ì•¼ í•¨
        assert chunk.rstrip().endswith(('.', '!', '?')), \
            f"ì²­í¬ê°€ ì™„ì „í•œ ë¬¸ì¥ìœ¼ë¡œ ëë‚˜ì§€ ì•ŠìŒ: {chunk[-50:]}"

        # ì²­í¬ ë‚´ì— ê³µë°±+ë§ˆì¹¨í‘œê°€ ì—†ì–´ì•¼ í•¨ (ë‹¨ì–´ ì ˆë‹¨ ë°©ì§€)
        assert ' .' not in chunk, \
            f"ë‹¨ì–´ ì ˆë‹¨ ê°ì§€: {chunk}"

def test_chunk_semantic_coherence():
    """ì²­í¬ì˜ ì˜ë¯¸ë¡ ì  ì¼ê´€ì„± ê²€ì¦"""

    chunker = SemanticChunker(chunk_size=600)
    embedding_service = EmbeddingService()

    long_text = load_test_document("medical_report.txt")  # 10MB

    chunks = chunker.chunk_text(long_text)

    # ê° ì²­í¬ì˜ ìì²´ ì¼ê´€ì„± ì¸¡ì •
    coherence_scores = []

    for chunk in chunks[:100]:  # ìƒ˜í”Œ 100ê°œ
        # ì²­í¬ë¥¼ ë¬¸ì¥ìœ¼ë¡œ ë¶„í• 
        sentences = chunker.split_into_sentences(chunk)

        if len(sentences) < 2:
            continue

        # ë¬¸ì¥ ê°„ ìœ ì‚¬ë„ ì¸¡ì •
        embeddings = embedding_service.embed(sentences)

        # ì—°ì†ëœ ë¬¸ì¥ ê°„ í‰ê·  ìœ ì‚¬ë„
        similarities = []
        for i in range(len(embeddings) - 1):
            sim = cosine_similarity(embeddings[i], embeddings[i+1])
            similarities.append(sim)

        coherence = sum(similarities) / len(similarities)
        coherence_scores.append(coherence)

    avg_coherence = sum(coherence_scores) / len(coherence_scores)

    # ì˜ë¯¸ë¡ ì  ì¼ê´€ì„±ì´ ë†’ì•„ì•¼ í•¨
    assert avg_coherence > 0.65, \
        f"ì²­í‚¹ ì˜ë¯¸ë¡ ì  ì¼ê´€ì„± ë‚®ìŒ: {avg_coherence:.3f}"

    log.info(f"ì²­í‚¹ ì¼ê´€ì„± ì ìˆ˜: {avg_coherence:.3f}")
```

### RAG ê²€ìƒ‰ ì •í™•ë„ í…ŒìŠ¤íŠ¸

```python
# tests/integration/test_rag_accuracy.py

def test_rag_retrieval_accuracy():
    """RAG ê²€ìƒ‰ ì •í™•ë„ ê²€ì¦"""

    # í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì—…ë¡œë“œ
    test_doc = "test_documents/technical_spec.pdf"  # 50MB
    pipeline.process_document(test_doc)

    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸-ë‹µë³€ ìŒ
    test_cases = [
        {
            "question": "ì‹œìŠ¤í…œì˜ ìµœëŒ€ ì²˜ë¦¬ ìš©ëŸ‰ì€?",
            "expected_keywords": ["100MB", "íŒŒì¼", "ì²˜ë¦¬"],
            "min_relevance": 0.75
        },
        {
            "question": "ë³´ì•ˆ ì¸ì¦ ë°©ì‹ì€ ë¬´ì—‡ì¸ê°€?",
            "expected_keywords": ["JWT", "ì¸ì¦", "ë³´ì•ˆ"],
            "min_relevance": 0.75
        },
        # ... ë” ë§ì€ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
    ]

    results = []

    for test_case in test_cases:
        # RAG ê²€ìƒ‰ ìˆ˜í–‰
        retrieved_docs = rag_service.retrieve_with_quality_filter(
            query=test_case["question"]
        )

        # ê´€ë ¨ì„± í™•ì¸
        assert len(retrieved_docs) > 0, "ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ"
        assert all(d.score >= test_case["min_relevance"] for d in retrieved_docs), \
            "ê²€ìƒ‰ ê´€ë ¨ì„± ë‚®ìŒ"

        # í‚¤ì›Œë“œ í¬í•¨ í™•ì¸
        retrieved_text = " ".join(d.payload["text"] for d in retrieved_docs)

        keyword_found = any(
            keyword in retrieved_text
            for keyword in test_case["expected_keywords"]
        )

        assert keyword_found, \
            f"ê¸°ëŒ€ í‚¤ì›Œë“œ ì—†ìŒ: {test_case['expected_keywords']}"

        results.append({
            "question": test_case["question"],
            "relevance": retrieved_docs[0].score,
            "passed": True
        })

    # ì „ì²´ ì •í™•ë„
    accuracy = len([r for r in results if r["passed"]]) / len(results)

    assert accuracy >= 0.85, f"RAG ì •í™•ë„ ë‚®ìŒ: {accuracy:.2%}"

    log.info(f"âœ… RAG ê²€ìƒ‰ ì •í™•ë„: {accuracy:.2%}")
```

---

## ğŸ” ëª¨ë‹ˆí„°ë§ ë° ë””ë²„ê¹…

### ì²˜ë¦¬ ë©”íŠ¸ë¦­ ë¡œê¹…

```python
# backend/app/middleware/document_metrics.py

class DocumentProcessingMetrics:
    """ë¬¸ì„œ ì²˜ë¦¬ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""

    def __init__(self):
        self.metrics = []

    def record_processing(
        self,
        file_size_mb: float,
        total_chunks: int,
        elapsed_seconds: float,
        peak_memory_mb: float
    ):
        """ì²˜ë¦¬ ë©”íŠ¸ë¦­ ê¸°ë¡"""

        metric = {
            "timestamp": datetime.now().isoformat(),
            "file_size_mb": file_size_mb,
            "total_chunks": total_chunks,
            "elapsed_seconds": elapsed_seconds,
            "throughput_mb_per_sec": file_size_mb / elapsed_seconds,
            "chunks_per_second": total_chunks / elapsed_seconds,
            "peak_memory_mb": peak_memory_mb,
            "memory_efficiency": file_size_mb / peak_memory_mb
        }

        self.metrics.append(metric)

        log.info(
            f"ğŸ“Š ì²˜ë¦¬ ë©”íŠ¸ë¦­: "
            f"{file_size_mb:.1f}MB â†’ {total_chunks} ì²­í¬ "
            f"({elapsed_seconds:.1f}ì´ˆ, {metric['throughput_mb_per_sec']:.2f} MB/s, "
            f"í”¼í¬ ë©”ëª¨ë¦¬: {peak_memory_mb:.0f}MB)"
        )

    def get_average_metrics(self) -> Dict:
        """í‰ê·  ë©”íŠ¸ë¦­ ê³„ì‚°"""

        if not self.metrics:
            return {}

        return {
            "avg_throughput": sum(m["throughput_mb_per_sec"] for m in self.metrics) / len(self.metrics),
            "avg_memory_efficiency": sum(m["memory_efficiency"] for m in self.metrics) / len(self.metrics),
            "total_processed_files": len(self.metrics),
            "total_processed_mb": sum(m["file_size_mb"] for m in self.metrics)
        }
```

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### êµ¬í˜„ ì™„ë£Œ í™•ì¸

- [x] Stage 1: ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ì¶”ì¶œ êµ¬í˜„
- [x] Stage 2: ì˜ë¯¸ë¡ ì  ì²­í‚¹ êµ¬í˜„
- [x] Stage 3: ë°°ì¹˜ ì„ë² ë”© ë° ì €ì¥ êµ¬í˜„
- [x] End-to-End íŒŒì´í”„ë¼ì¸ êµ¬í˜„
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ ì‘ì„± (50ê°œ ì´ìƒ)
- [ ] í†µí•© í…ŒìŠ¤íŠ¸ ì‘ì„± (10ê°œ ì´ìƒ)
- [ ] ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
- [ ] ì •í™•ë„ ê²€ì¦ ì‹¤í–‰
- [ ] ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ì‹¤í–‰
- [ ] ë¬¸ì„œí™” ì™„ë£Œ

### í’ˆì§ˆ ê¸°ì¤€

- [ ] 100MB íŒŒì¼ ì²˜ë¦¬ ì„±ê³µë¥  > 95%
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ < íŒŒì¼ í¬ê¸° Ã— 10
- [ ] ì²­í‚¹ ë¬¸ì¥ ì™„ì „ì„± = 100%
- [ ] RAG ê²€ìƒ‰ ì •í™•ë„ > 85%
- [ ] ì²˜ë¦¬ ì†ë„ > 0.3 MB/s

---

**ì‘ì„±ì¼**: 2025-10-30
**ë²„ì „**: 1.0
**ìƒíƒœ**: âœ… êµ¬í˜„ ê°€ì´ë“œ ì™„ë£Œ
